{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0469892b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Anirudh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import nltk, string\n",
    "import pickle, math\n",
    "from tqdm import tqdm\n",
    "\n",
    "var_dict = {}\n",
    "var_dict[\"D_SID\"] = '.I '\n",
    "var_dict[\"D_UI\"] = '.U'\n",
    "var_dict[\"D_MESH\"] = '.M'\n",
    "var_dict[\"D_TITLE\"] = '.T'\n",
    "var_dict[\"D_PUB\"] = '.P'\n",
    "var_dict[\"D_ABS\"] = '.W'\n",
    "var_dict[\"D_SOURCE\"] = '.S'\n",
    "var_dict[\"D_AUTHOR\"] = '.A'\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stpwrd = nltk.corpus.stopwords.words('english')\n",
    "stpwrd.extend(['.U', '.S','.M','.T','.P','.W','.M','.I'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "890a30a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_id(doc):\n",
    "    # Obtain string that contains the Document ID\n",
    "    id_string = doc[0]\n",
    "    # Split the string and make it a list\n",
    "    # the ID is the last element of that list\n",
    "    id_string = list(id_string.split(\"\\n\"))\n",
    "    return int(id_string[2])\n",
    "\n",
    "def remove_stopwords(text, stpwrd):\n",
    "    # Create a list of words without the stop words\n",
    "    word_list = [word for word in text.split() if word not in stpwrd]\n",
    "    # Create a string of the created list\n",
    "    text =  ' '.join(word_list) \n",
    "    return text\n",
    "\n",
    "def clean_text(input_text):\n",
    "    # Remove . Headers\n",
    "    input_text = re.sub(r'\\.[A-Z]', '', input_text)\n",
    "    # Remove punctuation\n",
    "    input_text = input_text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Remove \\n, \\r and \\t\n",
    "    input_text = re.sub(r'[\\n\\t\\r]', ' ', input_text)\n",
    "    # Remove digits\n",
    "    input_text = re.sub(r'[\\d]', '', input_text)\n",
    "    # Make text lower case\n",
    "    input_text = input_text.lower()\n",
    "    # Remove leading and trailing spaces\n",
    "    input_text = input_text.strip()\n",
    "    # Convert the text to a list of words\n",
    "    final_text = input_text.split()\n",
    "\n",
    "    return final_text\n",
    "\n",
    "def get_text(doc):\n",
    "    # Obtain string that contains the text \n",
    "    text_string = doc[1]\n",
    "    # Remove the stop words from the list\n",
    "    text_string = remove_stopwords(text_string, stpwrd)\n",
    "    # Process the string and convert it into a list of words\n",
    "    text_string = clean_text(text_string) \n",
    "    return text_string\n",
    "    \n",
    "def read_doc_file(path):\n",
    "    # Read the file and for each document, split the ID string and the text string\n",
    "    with open(path, 'r') as f:\n",
    "        documents = f.read().split(var_dict[\"D_SID\"]) \n",
    "    # Make a list for each file such that the ID and the text are separate elements\n",
    "    documents = [doc.split(\"\\n\"+var_dict[\"D_SOURCE\"], maxsplit=1) for doc in documents[1:]]\n",
    "    return documents\n",
    "\n",
    "def process_data(documents):\n",
    "    text_id_dict = {}\n",
    "    for doc in tqdm(documents):\n",
    "        try:\n",
    "            doc_id = get_id(doc)\n",
    "            doc_text = get_text(doc)\n",
    "#             print('----------')\n",
    "#             print(doc_text)\n",
    "#             print('----------')\n",
    "            text_id_dict[doc_id] = doc_text\n",
    "        except:\n",
    "            continue\n",
    "    return text_id_dict\n",
    "\n",
    "def index_generator(termlists):\n",
    "    # Empty dictionary to store the resulting indexes.\n",
    "    generated_index_list = {}\n",
    "    for filename, termlist in tqdm(termlists.items()):\n",
    "        # Empty dictionary to store the index for a file.\n",
    "        fileIndex = {}\n",
    "        for index, term in enumerate(termlist):\n",
    "            # checks whether it already exists as a key.\n",
    "            if term in fileIndex:\n",
    "                # Index is appended to existing list of positions.\n",
    "                fileIndex[term].append(index)\n",
    "            else:\n",
    "                # New key created for the term.\n",
    "                fileIndex[term] = [index]\n",
    "        # Add to index dictionary with the filename as the key.\n",
    "        generated_index_list[filename] = fileIndex\n",
    "    return generated_index_list\n",
    "\n",
    "def inverted_index_generator(generated_index_list):\n",
    "    # Empty dictionary to store the resulting inverted indexes.\n",
    "    inv_index_dict = {}\n",
    "    # Iterate through each file's index list.\n",
    "    for filename, file_index in tqdm(generated_index_list.items()):\n",
    "        # Iterate through each term in a file's index list\n",
    "        for term, pos in file_index.items():\n",
    "            # If the term is not present in the inverted index dict,\n",
    "            # add an empty key field to it.\n",
    "            if term not in inv_index_dict:\n",
    "                inv_index_dict[term] = {}\n",
    "            # If it is already present, add the filename and positions\n",
    "            # to the existing sub-dictionary.\n",
    "            inv_index_dict[term][filename] = pos\n",
    "    return inv_index_dict\n",
    "\n",
    "def generate_FTQ(curr_query, inverted_index):\n",
    "    # create a set to store the unique documents\n",
    "    set_of_docs = set()\n",
    "    # Iterate through each term in a query\n",
    "    for term in curr_query:\n",
    "        # Check if the term is present in the inverted index dictionary\n",
    "        if term in inverted_index_dict:\n",
    "            # If it is present, ger the unique docs\n",
    "            unique_docs = set(inverted_index_dict[term].keys())\n",
    "            # Update the set of documents\n",
    "            set_of_docs.update(unique_docs)\n",
    "    return list(set_of_docs)\n",
    "\n",
    "def parse_queries(file):\n",
    "    queries = []\n",
    "    current_query = None\n",
    "    for line in file:\n",
    "        line = line[:-1]\n",
    "        if '<top>' in line:\n",
    "            current_query = {}\n",
    "        elif '</top>' in line:\n",
    "            queries.append(current_query)\n",
    "            current_query = {}\n",
    "        elif '<num>' in line:\n",
    "            current_query['num'] = line.split(':')[1].strip()\n",
    "        elif '<title>' in line:\n",
    "            current_query['title'] = line.split('>')[1].strip()\n",
    "        elif (not '<desc>' in line and len(line) > 2):\n",
    "            current_query['description'] = line\n",
    "    return queries\n",
    "\n",
    "def extract_queries(queries):\n",
    "    # Remove the stopwords and clean the text\n",
    "    extracted_queries = {query['num']: clean_text(remove_stopwords(query['description'], stpwrd)) for query in queries}\n",
    "    return extracted_queries\n",
    "\n",
    "def query_reader(filename):\n",
    "    # Open the file and parse through the quaries\n",
    "    with open(filename, 'r+') as file:\n",
    "        queries = parse_queries(file)\n",
    "    # Extract and clean/process the queries\n",
    "    extracted_queries = extract_queries(queries)        \n",
    "    return extracted_queries\n",
    "\n",
    "def get_k_top(document_score, top_k):\n",
    "    # Convert the directory items to a list\n",
    "    docs_score_list = list(document_score.items())\n",
    "    # Sort the list according to the scores\n",
    "    sorted_docs_score_list = sorted(docs_score_list, key=lambda item: item[1], reverse=True)\n",
    "    # Pick the top scoring document\n",
    "    top_docs_score_list = sorted_docs_score_list[:top_k]\n",
    "    return top_docs_score_list\n",
    "\n",
    "def get_intersection_score(document, query):\n",
    "    doc_terms = index_dict[document].keys()\n",
    "    intersection = set(list(doc_terms)) & set(query)\n",
    "    boolean_score = len(list(intersection))\n",
    "    return boolean_score\n",
    "\n",
    "def get_boolean_rank(document_list, query, top_k):\n",
    "    document_score = {}\n",
    "    for document in document_list:\n",
    "        boolean_score = get_intersection_score(document, query)\n",
    "        document_score[document] = boolean_score\n",
    "    top_k_documents = get_k_top(document_score, top_k)\n",
    "    return top_k_documents\n",
    "\n",
    "def get_term_frequency(document, query):\n",
    "    curr_doc_len = len(index_dict[document])\n",
    "    frequency = 0\n",
    "    for term in query:\n",
    "        term_frequency = 0\n",
    "        if term in index_dict[document]:\n",
    "            term_frequency = len(index_dict[document][term])\n",
    "        frequency += term_frequency\n",
    "    return frequency/curr_doc_len\n",
    "\n",
    "def get_inv_doc_frequency(document, query):\n",
    "    curr_doc_len = len(index_dict[document])\n",
    "    cumulative_inv_doc_frequency = 0\n",
    "    for term in query:\n",
    "        if term in inverted_index_dict:\n",
    "            document_frequency = len(inverted_index_dict[term])\n",
    "            inv_doc_frequency = len(inverted_index_dict) / document_frequency\n",
    "            inv_doc_frequency = math.log(inv_doc_frequency)\n",
    "            cumulative_inv_doc_frequency += inv_doc_frequency\n",
    "    return cumulative_inv_doc_frequency\n",
    "\n",
    "def get_tf_rank(document_list, query, top_k):\n",
    "    document_score = {}\n",
    "    for document in document_list:\n",
    "        term_frequency = get_term_frequency(document, query)\n",
    "        document_score[document] = term_frequency\n",
    "    top_k_documents = get_k_top(document_score, top_k)\n",
    "    return top_k_documents\n",
    "\n",
    "def get_tf_idf_rank(document_list, query, top_k):\n",
    "    document_score = {}\n",
    "    for document in document_list:\n",
    "        term_frequency = get_term_frequency(document, query)\n",
    "        inv_doc_frequency = get_inv_doc_frequency(document, query)\n",
    "        tf_idf_score = term_frequency * inv_doc_frequency\n",
    "        document_score[document] = tf_idf_score\n",
    "    top_k_documents = get_k_top(document_score, top_k)\n",
    "    return top_k_documents\n",
    "\n",
    "def get_custom_rank(document_list, query, top_k):\n",
    "    discount_factor = 0.75\n",
    "    relevant_scores = get_tf_idf_rank(document_list, query, top_k)\n",
    "    relevant_documents = [doc for doc, score in relevant_scores]\n",
    "    document_score = {}\n",
    "    for count, document in enumerate(relevant_documents):\n",
    "        term_frequency = get_term_frequency(document, query)\n",
    "        inv_doc_frequency = get_inv_doc_frequency(document, query)\n",
    "        tf_idf_score = term_frequency * inv_doc_frequency\n",
    "        d_val = ((discount_factor)**count)\n",
    "        custom_score = (d_val + (tf_idf_score)) - math.log(tf_idf_score)\n",
    "        document_score[document] = custom_score\n",
    "    top_k_documents = get_k_top(document_score, top_k)\n",
    "    return top_k_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9c1cca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading documents...\n",
      "Preprocessing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 293867/293867 [01:35<00:00, 3072.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating indices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 293856/293856 [00:24<00:00, 12047.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating inverted indices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 293856/293856 [00:11<00:00, 26314.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the index file...\n",
      "Saving the inverted index file...\n"
     ]
    }
   ],
   "source": [
    "print('Reading documents...')\n",
    "documents = read_doc_file('ohsumed.88-91')\n",
    "\n",
    "print('Preprocessing data...')\n",
    "cleaned_data = process_data(documents)\n",
    "\n",
    "print('Generating indices...')\n",
    "index_dict = index_generator(cleaned_data)\n",
    "\n",
    "print('Generating inverted indices...')\n",
    "inv_index_dict = inverted_index_generator(index_dict)\n",
    "\n",
    "print('Saving the index file...')\n",
    "index_pickle = open(\"./pickle_files/index.pkl\", \"wb\")\n",
    "pickle.dump(index_dict, index_pickle)\n",
    "index_pickle.close()\n",
    "\n",
    "print('Saving the inverted index file...')\n",
    "inverted_index_pickle = open(\"./pickle_files/inverted_index.pkl\", \"wb\")\n",
    "pickle.dump(inv_index_dict, inverted_index_pickle)\n",
    "inverted_index_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b9ed69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = query_reader('query.ohsu.1-63')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce4b096",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file = open(\"./pickle_files/inverted_index.pkl\", \"rb\")\n",
    "inverted_index_dict = pickle.load(a_file)\n",
    "\n",
    "b_file = open(\"./pickle_files/index.pkl\", \"rb\")\n",
    "index_dict = pickle.load(b_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a68632a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose the ranking method: \n",
      " 1) Boolean\n",
      " 2) TF\n",
      " 3) TF-IDF\n",
      " 4) Custom\n",
      "\n",
      "Enter the method to be used: TF\n",
      "Generating Scores...\n",
      "Method: TF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Score generation complete...\n",
      "Please check the file in the output sub-directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_ranking_scores(method, inverted_index_dict, index_dict):\n",
    "    print('Generating Scores...')\n",
    "    print('Method:', method)\n",
    "    file_name = method + '.txt'\n",
    "    f = open('./output/'+file_name, 'w')\n",
    "    top_k = 50\n",
    "    for qid, query in tqdm(queries.items()):\n",
    "        document_list = generate_FTQ(query, inverted_index_dict)\n",
    "        if method == 'Boolean':\n",
    "            generated_scores = get_boolean_rank(document_list, query, top_k)\n",
    "        elif method == 'TF':\n",
    "            generated_scores = get_tf_rank(document_list, query, top_k)\n",
    "        elif method == 'TF-IDF':\n",
    "            generated_scores = get_tf_idf_rank(document_list, query, top_k)\n",
    "        elif method == 'Custom':\n",
    "            generated_scores = get_custom_rank(document_list, query, top_k)\n",
    "        total_scores = len(generated_scores)\n",
    "        for i in range(total_scores):\n",
    "            f.write(qid + \"\\tQ0\\t\" + str(generated_scores[i][0]) + \"\\t\" \n",
    "                    + str(i+1) + \"\\t\" + str(generated_scores[i][1]) + \"\\t\" + method +\"\\n\")\n",
    "            \n",
    "    print('\\n')\n",
    "    print('Score generation complete...')\n",
    "    print('Please check the file in the output sub-directory')\n",
    "\n",
    "            \n",
    "print('Choose the ranking method: \\n 1) Boolean\\n 2) TF\\n 3) TF-IDF\\n 4) Custom\\n')\n",
    "method = input('Enter the method to be used: ')\n",
    "get_ranking_scores(method, inverted_index_dict, index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be4ef413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_frequency(document):\n",
    "    curr_doc_len = len(index_dict[document])\n",
    "    frequency = 0\n",
    "    for term in query:\n",
    "        term_frequency = 0\n",
    "        if term in index_dict[document]:\n",
    "            term_frequency = len(index_dict[document][term])\n",
    "        frequency += term_frequency\n",
    "    return frequency/curr_doc_len\n",
    "\n",
    "def get_k_top(document_score, top_k):\n",
    "    # Convert the directory items to a list\n",
    "    docs_score_list = list(document_score.items())\n",
    "    # Sort the list according to the scores\n",
    "    sorted_docs_score_list = sorted(docs_score_list, key=lambda item: item[1], reverse=True)\n",
    "    # Pick the top scoring document\n",
    "    top_docs_score_list = sorted_docs_score_list[:top_k]\n",
    "    return top_docs_score_list\n",
    "\n",
    "def get_tf_rank(document_list, query, top_k):\n",
    "    document_score = {}\n",
    "    for document in document_list:\n",
    "        term_frequency = get_term_frequency(document)\n",
    "        document_score[document] = term_frequency\n",
    "    top_k_documents = get_k_top(document_score, top_k)\n",
    "    return top_k_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d3920108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_top(document_score, top_k):\n",
    "    docs_score_list = list(document_score.items())\n",
    "    sorted_docs_score_list = sorted(docs_score_list, key=lambda item: item[1], reverse=True)\n",
    "    top_docs_score_list = sorted_docs_score_list[:top_k]\n",
    "    return top_docs_score_list\n",
    "\n",
    "def get_intersection_score(document, query):\n",
    "    doc_terms = index_dict[document].keys()\n",
    "    intersection = set(list(doc_terms)) & set(query)\n",
    "    boolean_score = len(list(intersection))\n",
    "    return boolean_score\n",
    "\n",
    "def get_boolean_rank(document_list, query, top_k):\n",
    "    document_score = {}\n",
    "    for document in document_list:\n",
    "        boolean_score = get_intersection_score(document, query)\n",
    "        document_score[document] = boolean_score\n",
    "    top_k_documents = get_k_top(document_score, top_k)\n",
    "    return top_k_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "19d05561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_frequency(document):\n",
    "    curr_doc_len = len(index_dict[document])\n",
    "    frequency = 0\n",
    "    for term in query:\n",
    "        term_frequency = 0\n",
    "        if term in index_dict[document]:\n",
    "            term_frequency = len(index_dict[document][term])\n",
    "        frequency += term_frequency\n",
    "    return frequency/curr_doc_len\n",
    "\n",
    "def get_inv_doc_frequency(document):\n",
    "    curr_doc_len = len(index_dict[document])\n",
    "    cumulative_inv_doc_frequency = 0\n",
    "    for term in query:\n",
    "        if term in inverted_index_dict:\n",
    "            document_frequency = len(inverted_index_dict[term])\n",
    "            inv_doc_frequency = len(inverted_index_dict) / document_frequency\n",
    "            inv_doc_frequency = math.log(inv_doc_frequency)\n",
    "            cumulative_inv_doc_frequency += inv_doc_frequency\n",
    "    return cumulative_inv_doc_frequency\n",
    "\n",
    "def get_k_top(document_score, top_k):\n",
    "    docs_score_list = list(document_score.items())\n",
    "    sorted_docs_score_list = sorted(docs_score_list, key=lambda item: item[1], reverse=True)\n",
    "    top_docs_score_list = sorted_docs_score_list[:top_k]\n",
    "    return top_docs_score_list\n",
    "\n",
    "def get_tf_idf_rank(document_list, query, top_k):\n",
    "    document_score = {}\n",
    "    for document in document_list:\n",
    "        term_frequency = get_term_frequency(document)\n",
    "        inv_doc_frequency = get_inv_doc_frequency(document)\n",
    "        tf_idf_score = term_frequency * inv_doc_frequency\n",
    "        document_score[document] = tf_idf_score\n",
    "    top_k_documents = get_k_top(document_score, top_k)\n",
    "    return top_k_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "07a5e5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_custom_rank(document_list, query, top_k):\n",
    "    discount_factor = 0.75\n",
    "    relevant_scores = get_tf_idf_rank(docs, query, 50)\n",
    "    relevant_documents = [doc for doc, score in relevant_scores]\n",
    "    document_score = {}\n",
    "    for count, document in enumerate(relevant_documents):\n",
    "        term_frequency = get_term_frequency(document)\n",
    "        inv_doc_frequency = get_inv_doc_frequency(document)\n",
    "        tf_idf_score = term_frequency * inv_doc_frequency\n",
    "        d_val = ((discount_factor)**count)\n",
    "        custom_score = (d_val + (tf_idf_score)) - math.log(tf_idf_score)\n",
    "        document_score[document] = custom_score\n",
    "    top_k_documents = get_k_top(document_score, top_k)\n",
    "    return top_k_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ecf14d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "for query_code, query in queries.items():\n",
    "    docs = generate_FTQ(query, inverted_index_dict)\n",
    "    docs_score = get_custom_rank(docs, query, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e89c3191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(91255752, 6.62219744337424),\n",
       " (91295700, 6.070629253065326),\n",
       " (89363736, 5.883129253065325),\n",
       " (90319778, 5.742504253065325),\n",
       " (91333684, 5.4970671961436075),\n",
       " (88310191, 5.035790957153294),\n",
       " (88287652, 4.976464785278294),\n",
       " (91215324, 4.931970156372044),\n",
       " (91218591, 4.8985991846923564),\n",
       " (89002042, 4.438175358650875),\n",
       " (91296148, 4.419404187081051),\n",
       " (91167805, 4.405325808403683),\n",
       " (88063114, 4.203423661287281),\n",
       " (91124570, 4.019151647153143),\n",
       " (91269932, 3.9301495328432434),\n",
       " (90074399, 3.8773882933876944),\n",
       " (89001955, 3.691512823841468),\n",
       " (90382798, 3.689007174902063),\n",
       " (90023136, 3.6871279381975097),\n",
       " (90355638, 3.6857185106690946),\n",
       " (90066683, 3.6846614400227833),\n",
       " (88073722, 3.68386863703805),\n",
       " (91369527, 3.412777657716157),\n",
       " (91213849, 3.4123317060372442),\n",
       " (89162735, 3.31131855524717),\n",
       " (91325316, 3.29022946302501),\n",
       " (90313621, 3.2900413271604685),\n",
       " (89135238, 3.289900225262063),\n",
       " (91046708, 3.2897943988382585),\n",
       " (90135678, 3.2897150290204054),\n",
       " (89218414, 3.289655501657015),\n",
       " (89353008, 3.289610856134473),\n",
       " (89156406, 3.2895773719925656),\n",
       " (89026234, 3.2285219002465624),\n",
       " (91369519, 3.203712171392521),\n",
       " (91217924, 3.203698045270154),\n",
       " (91163618, 3.1760312287502845),\n",
       " (90253939, 3.0513064993748493),\n",
       " (91226301, 2.9704032679956542),\n",
       " (91232303, 2.9703987984022495),\n",
       " (91069448, 2.9703954462071955),\n",
       " (91069471, 2.9703929320609044),\n",
       " (91137922, 2.970391046451187),\n",
       " (88189265, 2.970389632243899),\n",
       " (91073174, 2.9703885715884324),\n",
       " (89013601, 2.9703877760968327),\n",
       " (88260541, 2.9703871794781325),\n",
       " (91242893, 2.9703867320141084),\n",
       " (88327364, 2.9703863964160897),\n",
       " (91212148, 2.9703861447175752)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bf45f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_ranking(docs, query):\n",
    "    docs_score = {}\n",
    "    for doc in docs:\n",
    "        doc_length = len(index_dict[doc])\n",
    "        absolute_freq = 0\n",
    "        normalized_freq = 0\n",
    "        for term in query:\n",
    "            try:\n",
    "                term_freq = len(index_dict[doc][term])\n",
    "            except:\n",
    "                term_freq = 0\n",
    "            absolute_freq += term_freq\n",
    "        normalized_freq = absolute_freq / doc_length\n",
    "        docs_score[doc] = normalized_freq\n",
    "    docs_score = list( sorted(docs_score.items(),\n",
    "                           key=lambda item: item[1],\n",
    "                           reverse=True))\n",
    "    return docs_score[:50]\n",
    "\n",
    "for query_code, query in queries.items():\n",
    "    docs = generate_FTQ(query, inverted_index_dict)\n",
    "    docs_score_1 = tf_ranking(docs, query)\n",
    "    docs_score_2 = get_tf_rank(docs, query, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a091d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(91255752, 0.2857142857142857),\n",
       " (91295700, 0.2727272727272727),\n",
       " (89363736, 0.2727272727272727),\n",
       " (90319778, 0.2727272727272727),\n",
       " (91333684, 0.26666666666666666),\n",
       " (88310191, 0.25),\n",
       " (88287652, 0.25),\n",
       " (91215324, 0.25),\n",
       " (91218591, 0.25),\n",
       " (89002042, 0.23076923076923078),\n",
       " (91296148, 0.23076923076923078),\n",
       " (91167805, 0.23076923076923078),\n",
       " (88063114, 0.2222222222222222),\n",
       " (91124570, 0.21428571428571427),\n",
       " (91269932, 0.21052631578947367),\n",
       " (90074399, 0.20833333333333334),\n",
       " (89001955, 0.2),\n",
       " (90382798, 0.2),\n",
       " (90023136, 0.2),\n",
       " (90355638, 0.2),\n",
       " (90066683, 0.2),\n",
       " (88073722, 0.2),\n",
       " (91369527, 0.1875),\n",
       " (91213849, 0.1875),\n",
       " (89162735, 0.1827956989247312),\n",
       " (91325316, 0.18181818181818182),\n",
       " (90313621, 0.18181818181818182),\n",
       " (89135238, 0.18181818181818182),\n",
       " (91046708, 0.18181818181818182),\n",
       " (90135678, 0.18181818181818182),\n",
       " (89218414, 0.18181818181818182),\n",
       " (89353008, 0.18181818181818182),\n",
       " (89156406, 0.18181818181818182),\n",
       " (89026234, 0.17894736842105263),\n",
       " (91369519, 0.17777777777777778),\n",
       " (91217924, 0.17777777777777778),\n",
       " (91163618, 0.17647058823529413),\n",
       " (90253939, 0.17054263565891473),\n",
       " (91226301, 0.16666666666666666),\n",
       " (91232303, 0.16666666666666666),\n",
       " (91069448, 0.16666666666666666),\n",
       " (91069471, 0.16666666666666666),\n",
       " (91137922, 0.16666666666666666),\n",
       " (88189265, 0.16666666666666666),\n",
       " (91073174, 0.16666666666666666),\n",
       " (89013601, 0.16666666666666666),\n",
       " (88260541, 0.16666666666666666),\n",
       " (91242893, 0.16666666666666666),\n",
       " (88327364, 0.16666666666666666),\n",
       " (91212148, 0.16666666666666666)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ff4e6659",
   "metadata": {},
   "outputs": [],
   "source": [
    "for query_code, query in queries.items():\n",
    "    docs = generate_FTQ(query, inverted_index_dict)\n",
    "    docs_score_1 = tf_idf_ranking(docs, query)\n",
    "    docs_score_2 = get_tf_idf_rank(docs, query, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ab14a9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_score_1 == docs_score_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9e641175",
   "metadata": {},
   "outputs": [],
   "source": [
    "for query_code, query in queries.items():\n",
    "    docs = generate_FTQ(query, inverted_index_dict)\n",
    "    docs_score_bool = boolean_ranking(docs, query)\n",
    "    docs_score_bool2 = get_boolean_rank(docs, query, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "626a5c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_score_bool == docs_score_bool2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3bcfffc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(91255752, 7.657940530763741),\n",
       " (91295700, 7.309852324819935),\n",
       " (89363736, 7.309852324819935),\n",
       " (90319778, 7.309852324819935),\n",
       " (91333684, 7.147411162046159),\n",
       " (88310191, 6.700697964418274),\n",
       " (88287652, 6.700697964418274),\n",
       " (91215324, 6.700697964418274),\n",
       " (91218591, 6.700697964418274),\n",
       " (89002042, 6.185259659463022),\n",
       " (91296148, 6.185259659463022),\n",
       " (91167805, 6.185259659463022),\n",
       " (88063114, 5.956175968371799),\n",
       " (91124570, 5.743455398072806),\n",
       " (91269932, 5.64269302266802),\n",
       " (90074399, 5.5839149703485615),\n",
       " (89001955, 5.360558371534619),\n",
       " (90382798, 5.360558371534619),\n",
       " (90023136, 5.360558371534619),\n",
       " (90355638, 5.360558371534619),\n",
       " (90066683, 5.360558371534619),\n",
       " (88073722, 5.360558371534619),\n",
       " (91369527, 5.025523473313705),\n",
       " (91213849, 5.025523473313705),\n",
       " (89162735, 4.899435070757447),\n",
       " (91325316, 4.87323488321329),\n",
       " (90313621, 4.87323488321329),\n",
       " (89135238, 4.87323488321329),\n",
       " (91046708, 4.87323488321329),\n",
       " (90135678, 4.87323488321329),\n",
       " (89218414, 4.87323488321329),\n",
       " (89353008, 4.87323488321329),\n",
       " (89156406, 4.87323488321329),\n",
       " (89026234, 4.796289069267817),\n",
       " (91369519, 4.76494077469744),\n",
       " (91217924, 4.76494077469744),\n",
       " (91163618, 4.729904445471723),\n",
       " (90253939, 4.571018766424869),\n",
       " (91226301, 4.467131976278849),\n",
       " (91232303, 4.467131976278849),\n",
       " (91069448, 4.467131976278849),\n",
       " (91069471, 4.467131976278849),\n",
       " (91137922, 4.467131976278849),\n",
       " (88189265, 4.467131976278849),\n",
       " (91073174, 4.467131976278849),\n",
       " (89013601, 4.467131976278849),\n",
       " (88260541, 4.467131976278849),\n",
       " (91242893, 4.467131976278849),\n",
       " (88327364, 4.467131976278849),\n",
       " (91212148, 4.467131976278849)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_score_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c70a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
