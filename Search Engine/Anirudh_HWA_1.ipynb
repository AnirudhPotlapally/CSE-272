{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0469892b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Anirudh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import nltk, string\n",
    "import pickle, math\n",
    "from tqdm import tqdm\n",
    "\n",
    "var_dict = {}\n",
    "var_dict[\"D_SID\"] = '.I '\n",
    "var_dict[\"D_UI\"] = '.U'\n",
    "var_dict[\"D_MESH\"] = '.M'\n",
    "var_dict[\"D_TITLE\"] = '.T'\n",
    "var_dict[\"D_PUB\"] = '.P'\n",
    "var_dict[\"D_ABS\"] = '.W'\n",
    "var_dict[\"D_SOURCE\"] = '.S'\n",
    "var_dict[\"D_AUTHOR\"] = '.A'\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stpwrd = nltk.corpus.stopwords.words('english')\n",
    "stpwrd.extend(['.U', '.S','.M','.T','.P','.W','.M','.I'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "890a30a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#############################################################################################################################\n",
    "############################### FUNCTIONS TO PROCESS AND CLEAN THE DATA #####################################################\n",
    "#############################################################################################################################\n",
    "\n",
    "def get_id(doc):\n",
    "    # Obtain string that contains the Document ID\n",
    "    id_string = doc[0]\n",
    "    # Split the string and make it a list\n",
    "    # the ID is the last element of that list\n",
    "    id_string = list(id_string.split(\"\\n\"))\n",
    "    return int(id_string[2])\n",
    "\n",
    "def remove_stopwords(text, stpwrd):\n",
    "    # Create a list of words without the stop words\n",
    "    word_list = [word for word in text.split() if word not in stpwrd]\n",
    "    # Create a string of the created list\n",
    "    text =  ' '.join(word_list) \n",
    "    return text\n",
    "\n",
    "def clean_text(input_text):\n",
    "    # Remove . Headers\n",
    "    input_text = re.sub(r'\\.[A-Z]', '', input_text)\n",
    "    # Remove punctuation\n",
    "    input_text = input_text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Remove \\n, \\r and \\t\n",
    "    input_text = re.sub(r'[\\n\\t\\r]', ' ', input_text)\n",
    "    # Remove digits\n",
    "    input_text = re.sub(r'[\\d]', '', input_text)\n",
    "    # Make text lower case\n",
    "    input_text = input_text.lower()\n",
    "    # Remove leading and trailing spaces\n",
    "    input_text = input_text.strip()\n",
    "    # Convert the text to a list of words\n",
    "    final_text = input_text.split()\n",
    "\n",
    "    return final_text\n",
    "\n",
    "def get_text(doc):\n",
    "    # Obtain string that contains the text \n",
    "    text_string = doc[1]\n",
    "    # Remove the stop words from the list\n",
    "    text_string = remove_stopwords(text_string, stpwrd)\n",
    "    # Process the string and convert it into a list of words\n",
    "    text_string = clean_text(text_string) \n",
    "    return text_string\n",
    "    \n",
    "def read_doc_file(path):\n",
    "    # Read the file and for each document, split the ID string and the text string\n",
    "    with open(path, 'r') as f:\n",
    "        documents = f.read().split(var_dict[\"D_SID\"]) \n",
    "    # Make a list for each file such that the ID and the text are separate elements\n",
    "    documents = [doc.split(\"\\n\"+var_dict[\"D_SOURCE\"], maxsplit=1) for doc in documents[1:]]\n",
    "    return documents\n",
    "\n",
    "def process_data(documents):\n",
    "    text_id_dict = {}\n",
    "    for doc in tqdm(documents):\n",
    "        try:\n",
    "            doc_id = get_id(doc)\n",
    "            doc_text = get_text(doc)\n",
    "#             print('----------')\n",
    "#             print(doc_text)\n",
    "#             print('----------')\n",
    "            text_id_dict[doc_id] = doc_text\n",
    "        except:\n",
    "            continue\n",
    "    return text_id_dict\n",
    "\n",
    "#############################################################################################################################\n",
    "############################### FUNCTIONS TO GENERATE DOCUMENT INDICES AND INVERTED INDICES #################################\n",
    "#############################################################################################################################\n",
    "\n",
    "def index_generator(termlists):\n",
    "    # Empty dictionary to store the resulting indexes.\n",
    "    generated_index_list = {}\n",
    "    for filename, termlist in tqdm(termlists.items()):\n",
    "        # Empty dictionary to store the index for a file.\n",
    "        fileIndex = {}\n",
    "        for index, term in enumerate(termlist):\n",
    "            # checks whether it already exists as a key.\n",
    "            if term in fileIndex:\n",
    "                # Index is appended to existing list of positions.\n",
    "                fileIndex[term].append(index)\n",
    "            else:\n",
    "                # New key created for the term.\n",
    "                fileIndex[term] = [index]\n",
    "        # Add to index dictionary with the filename as the key.\n",
    "        generated_index_list[filename] = fileIndex\n",
    "    return generated_index_list\n",
    "\n",
    "def inverted_index_generator(generated_index_list):\n",
    "    # Empty dictionary to store the resulting inverted indexes.\n",
    "    inv_index_dict = {}\n",
    "    # Iterate through each file's index list.\n",
    "    for filename, file_index in tqdm(generated_index_list.items()):\n",
    "        # Iterate through each term in a file's index list\n",
    "        for term, pos in file_index.items():\n",
    "            # If the term is not present in the inverted index dict,\n",
    "            # add an empty key field to it.\n",
    "            if term not in inv_index_dict:\n",
    "                inv_index_dict[term] = {}\n",
    "            # If it is already present, add the filename and positions\n",
    "            # to the existing sub-dictionary.\n",
    "            inv_index_dict[term][filename] = pos\n",
    "    return inv_index_dict\n",
    "\n",
    "#############################################################################################################################\n",
    "############################### FUNCTIONS TO PARSE AND PROCESS THE QUERIES ##################################################\n",
    "#############################################################################################################################\n",
    "\n",
    "def parse_queries(file):\n",
    "    queries = []\n",
    "    current_query = None\n",
    "    for line in file:\n",
    "        line = line[:-1]\n",
    "        if '<top>' in line:\n",
    "            current_query = {}\n",
    "        elif '</top>' in line:\n",
    "            queries.append(current_query)\n",
    "            current_query = {}\n",
    "        elif '<num>' in line:\n",
    "            current_query['num'] = line.split(':')[1].strip()\n",
    "        elif '<title>' in line:\n",
    "            current_query['title'] = line.split('>')[1].strip()\n",
    "        elif (not '<desc>' in line and len(line) > 2):\n",
    "            current_query['description'] = line\n",
    "    return queries\n",
    "\n",
    "def generate_FTQ(curr_query, inverted_index_dict):\n",
    "    # create a set to store the unique documents\n",
    "    set_of_docs = set()\n",
    "    # Iterate through each term in a query\n",
    "    for term in curr_query:\n",
    "        # Check if the term is present in the inverted index dictionary\n",
    "        if term in inverted_index_dict:\n",
    "            # If it is present, ger the unique docs\n",
    "            unique_docs = set(inverted_index_dict[term].keys())\n",
    "            # Update the set of documents\n",
    "            set_of_docs.update(unique_docs)\n",
    "    return list(set_of_docs)\n",
    "\n",
    "def extract_queries(queries):\n",
    "    # Remove the stopwords and clean the text\n",
    "    extracted_queries = {query['num']: clean_text(remove_stopwords(query['description'], stpwrd)) for query in queries}\n",
    "    return extracted_queries\n",
    "\n",
    "def query_reader(filename):\n",
    "    # Open the file and parse through the quaries\n",
    "    with open(filename, 'r+') as file:\n",
    "        queries = parse_queries(file)\n",
    "    # Extract and clean/process the queries\n",
    "    extracted_queries = extract_queries(queries)        \n",
    "    return extracted_queries\n",
    "\n",
    "#############################################################################################################################\n",
    "############################### FUNCTIONS TO OBRAIN THE RANKING SCORES ######################################################\n",
    "#############################################################################################################################\n",
    "\n",
    "def get_k_top(document_score, top_k):\n",
    "    # Convert the directory items to a list\n",
    "    docs_score_list = list(document_score.items())\n",
    "    # Sort the list according to the scores\n",
    "    sorted_docs_score_list = sorted(docs_score_list, key=lambda item: item[1], reverse=True)\n",
    "    # Pick the top scoring document\n",
    "    top_docs_score_list = sorted_docs_score_list[:top_k]\n",
    "    return top_docs_score_list\n",
    "\n",
    "def get_intersection_score(document, query):\n",
    "    # Get the document terms from the index dictionary\n",
    "    doc_terms = index_dict[document].keys()\n",
    "    # Get the intersection between the query terms and the document terms\n",
    "    intersection = set(list(doc_terms)) & set(query)\n",
    "    # The length of the intersection gives the boolean score\n",
    "    boolean_score = len(list(intersection))\n",
    "    return boolean_score\n",
    "\n",
    "def get_boolean_rank(document_list, query, top_k):\n",
    "    # Dictionary to store the document scores\n",
    "    document_score = {}\n",
    "    # Iterate through each document\n",
    "    for document in document_list:\n",
    "        # Get the boolean score\n",
    "        boolean_score = get_intersection_score(document, query)\n",
    "        # Store the score in the dictionary\n",
    "        document_score[document] = boolean_score\n",
    "    # Get the top K relevant documents\n",
    "    top_k_documents = get_k_top(document_score, top_k)\n",
    "    return top_k_documents\n",
    "\n",
    "def get_term_frequency(document, query):\n",
    "    curr_doc_len = len(index_dict[document])\n",
    "    frequency = 0\n",
    "    # Iterate through each term in the query\n",
    "    for term in query:\n",
    "        term_frequency = 0\n",
    "        # Check if the term is in the index dictionary\n",
    "        if term in index_dict[document]:\n",
    "            # Check its occurance\n",
    "            term_frequency = len(index_dict[document][term])\n",
    "        # Update term frequency\n",
    "        frequency += term_frequency\n",
    "    return frequency/curr_doc_len\n",
    "\n",
    "def get_inv_doc_frequency(document, query):\n",
    "    curr_doc_len = len(index_dict[document])\n",
    "    cumulative_inv_doc_frequency = 0\n",
    "    # Iterate through each term in the query\n",
    "    for term in query:\n",
    "        # Check if the term is in the inverted index dictionary\n",
    "        if term in inverted_index_dict:\n",
    "            # Calculate the inverted document frequency\n",
    "            document_frequency = len(inverted_index_dict[term])\n",
    "            inv_doc_frequency = len(inverted_index_dict) / document_frequency\n",
    "            inv_doc_frequency = math.log(inv_doc_frequency)\n",
    "            cumulative_inv_doc_frequency += inv_doc_frequency\n",
    "    return cumulative_inv_doc_frequency\n",
    "\n",
    "def get_tf_rank(document_list, query, top_k):\n",
    "    document_score = {}\n",
    "    # Iterate through each document in the document list\n",
    "    for document in document_list:\n",
    "        # Get the term frequency\n",
    "        term_frequency = get_term_frequency(document, query)\n",
    "        # Add it to the document score\n",
    "        document_score[document] = term_frequency\n",
    "    # Choose the top K relevant documents\n",
    "    top_k_documents = get_k_top(document_score, top_k)\n",
    "    return top_k_documents\n",
    "\n",
    "def get_tf_idf_rank(document_list, query, top_k):\n",
    "    document_score = {}\n",
    "    # Iterate through each document in the document list\n",
    "    for document in document_list:\n",
    "        # Get the term frequency\n",
    "        term_frequency = get_term_frequency(document, query)\n",
    "        # Get the inverted document frequency\n",
    "        inv_doc_frequency = get_inv_doc_frequency(document, query)\n",
    "        tf_idf_score = term_frequency * inv_doc_frequency\n",
    "        # Add it to the document score\n",
    "        document_score[document] = tf_idf_score\n",
    "    # Choose the top K relevant documents\n",
    "    top_k_documents = get_k_top(document_score, top_k)\n",
    "    return top_k_documents\n",
    "\n",
    "def get_custom_rank(document_list, query, top_k):\n",
    "    # Discount factor \n",
    "    discount_factor = 0.75\n",
    "    # Get initial set of relevant documents using TF-IDF\n",
    "    relevant_scores = get_tf_idf_rank(document_list, query, top_k)\n",
    "    relevant_documents = [doc for doc, score in relevant_scores]\n",
    "    document_score = {}\n",
    "    # Iterate through each document in the relevant document list \n",
    "    for count, document in enumerate(relevant_documents):\n",
    "        # Get the term frequency \n",
    "        term_frequency = get_term_frequency(document, query)\n",
    "        # Get the inverted document frequency\n",
    "        inv_doc_frequency = get_inv_doc_frequency(document, query)\n",
    "        tf_idf_score = term_frequency * inv_doc_frequency\n",
    "        # Discount value keeps reducing as the relevance of the document reduces\n",
    "        d_val = ((discount_factor)**count)\n",
    "        # Get the custom score\n",
    "        custom_score = (d_val + (tf_idf_score)) - math.log(tf_idf_score)\n",
    "        # Add it to the document score\n",
    "        document_score[document] = custom_score\n",
    "    # Choose the top K relevant documents\n",
    "    top_k_documents = get_k_top(document_score, top_k)\n",
    "    return top_k_documents\n",
    "\n",
    "#############################################################################################################################\n",
    "#############################################################################################################################\n",
    "#############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9c1cca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading documents...\n",
      "Preprocessing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 293867/293867 [01:37<00:00, 2999.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating indices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 293856/293856 [00:23<00:00, 12736.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating inverted indices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 293856/293856 [00:11<00:00, 25497.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the index file...\n",
      "Saving the inverted index file...\n"
     ]
    }
   ],
   "source": [
    "print('Reading documents...')\n",
    "documents = read_doc_file('ohsumed.88-91')\n",
    "\n",
    "print('Preprocessing data...')\n",
    "cleaned_data = process_data(documents)\n",
    "\n",
    "print('Generating indices...')\n",
    "index_dict = index_generator(cleaned_data)\n",
    "\n",
    "print('Generating inverted indices...')\n",
    "inv_index_dict = inverted_index_generator(index_dict)\n",
    "\n",
    "print('Saving the index file...')\n",
    "index_pickle = open(\"./pickle_files/index.pkl\", \"wb\")\n",
    "pickle.dump(index_dict, index_pickle)\n",
    "index_pickle.close()\n",
    "\n",
    "print('Saving the inverted index file...')\n",
    "inverted_index_pickle = open(\"./pickle_files/inverted_index.pkl\", \"wb\")\n",
    "pickle.dump(inv_index_dict, inverted_index_pickle)\n",
    "inverted_index_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "653c6ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = query_reader('query.ohsu.1-63')\n",
    "inv_index_file = open(\"./pickle_files/inverted_index.pkl\", \"rb\")\n",
    "inverted_index_dict = pickle.load(inv_index_file)\n",
    "index_file = open(\"./pickle_files/index.pkl\", \"rb\")\n",
    "index_dict = pickle.load(index_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9712856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose the ranking method: \n",
      " 1) Boolean\n",
      " 2) TF\n",
      " 3) TF-IDF\n",
      " 4) Custom\n",
      "\n",
      "Enter the method to be used: Custom\n",
      "Generating Scores...\n",
      "Method: Custom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [00:15<00:00,  4.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Score generation complete...\n",
      "Please check the file in the output sub-directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_ranking_scores(method, inverted_index_dict, index_dict):\n",
    "    print('Generating Scores...')\n",
    "    print('Method:', method)\n",
    "    file_name = method + '.txt'\n",
    "    f = open('./output/'+file_name, 'w')\n",
    "    top_k = 50\n",
    "    for qid, query in tqdm(queries.items()):\n",
    "        document_list = generate_FTQ(query, inverted_index_dict)\n",
    "        if method == 'Boolean':\n",
    "            generated_scores = get_boolean_rank(document_list, query, top_k)\n",
    "        elif method == 'TF':\n",
    "            generated_scores = get_tf_rank(document_list, query, top_k)\n",
    "        elif method == 'TF-IDF':\n",
    "            generated_scores = get_tf_idf_rank(document_list, query, top_k)\n",
    "        elif method == 'Custom':\n",
    "            generated_scores = get_custom_rank(document_list, query, top_k)\n",
    "        total_scores = len(generated_scores)\n",
    "        for i in range(total_scores):\n",
    "            f.write(qid + \"\\tQ0\\t\" + str(generated_scores[i][0]) + \"\\t\" + str(i+1) + \"\\t\" + str(generated_scores[i][1]) + \"\\t\" + method +\"\\n\")      \n",
    "    print('\\n')\n",
    "    print('Score generation complete...')\n",
    "    print('Please check the file in the output sub-directory')\n",
    "\n",
    "print('Choose the ranking method: \\n 1) Boolean\\n 2) TF\\n 3) TF-IDF\\n 4) Custom\\n')\n",
    "method = input('Enter the method to be used: ')\n",
    "get_ranking_scores(method, inverted_index_dict, index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eeb92e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
