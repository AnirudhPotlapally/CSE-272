{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30688b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD, BaselineOnly, NMF, SlopeOne, CoClustering \n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from sklearn.pipeline import Pipeline\n",
    "from surprise.model_selection import cross_validate, train_test_split, GridSearchCV\n",
    "from surprise import accuracy\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import gzip, pickle\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d7886d",
   "metadata": {},
   "source": [
    "## Data Selection Step:\n",
    "\n",
    "\n",
    "1. **Read Data from GZ File**: In this step, we read the data from a GZ file line by line and store each line in a list.\n",
    "\n",
    "2. **Convert List to DataFrame**: After reading the data, we convert the list into a dataframe. A dataframe is a tabular data structure commonly used in data analysis and manipulation.\n",
    "\n",
    "3. **Column Selection**: Once we have the dataframe, we identify and drop the unwanted columns that are not required for further analysis. \n",
    "\n",
    "4. **Choose Relevant Data Fields**: From the remaining columns, we select three specific data fields: 'ReviewerID', 'asin', and 'overall'. These fields provide information about the reviewer's ID, the product's ASIN (Amazon Standard Identification Number), and the overall rating given by the reviewer.\n",
    "\n",
    "5. **Drop Null Fields**: We identify and drop any null fields in the dataframe. Null fields are missing or undefined values that could affect the accuracy of our analysis.\n",
    "\n",
    "6. **Reduce Dataset Size**: Since the dataset is extremely big, for ease of calcluation, a subset of 100,000 data samples were chosen the dataset for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bd43ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datafile):\n",
    "    data = []\n",
    "    with gzip.open(datafile) as f:\n",
    "        for l in f:\n",
    "            data.append(json.loads(l.strip()))\n",
    "    print('Finished readind data from the file...')\n",
    "    print('Proceeding to convert the data into a dataframe')\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "datafile = 'Automotive.json.gz'\n",
    "# df = load_data(datafile)\n",
    "\n",
    "# Dropping unwanted columns\n",
    "# df.drop(['verified', 'reviewTime', 'style','reviewText', 'reviewerName', 'summary', 'unixReviewTime', 'vote', 'image'], axis=1, inplace=True)\n",
    "\n",
    "# Dropping fields which are NULL\n",
    "# df = df.dropna()\n",
    "\n",
    "# Saving that dataset as a csv file for further use\n",
    "# df.to_csv('./data/dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92f34edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of ratings:  7990166\n",
      "Number of unique reviewers:  3873247\n",
      "Number of unique products:  925387\n"
     ]
    }
   ],
   "source": [
    "total_dataset = pd.read_csv('./data/dataset.csv')\n",
    "\n",
    "total_ratings = total_dataset.shape[0]\n",
    "print(\"Total Number of ratings: \", total_ratings)\n",
    "\n",
    "total_unique_reviwer_id, total_reviewer_count = np.unique(total_dataset.reviewerID, return_counts = True)\n",
    "print(\"Number of unique reviewers: \", len(total_unique_reviwer_id))\n",
    "\n",
    "\n",
    "total_unique_product_id, total_product_count = np.unique(total_dataset.asin, return_counts = True)\n",
    "print(\"Number of unique products: \", len(total_unique_product_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "880dfbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique reviewers:  96805\n",
      "Number of unique products:  61933\n"
     ]
    }
   ],
   "source": [
    "dataset = total_dataset.sample(n=100000) \n",
    "\n",
    "unique_reviwer_id, reviewer_count = np.unique(dataset.reviewerID, return_counts = True)\n",
    "print(\"Number of unique reviewers: \", len(unique_reviwer_id))\n",
    "\n",
    "unique_product_id, product_count = np.unique(dataset.asin, return_counts = True)\n",
    "print(\"Number of unique products: \", len(unique_product_id))\n",
    "\n",
    "dataset.to_csv('./data/final_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47e583ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('./data/final_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50e87f3",
   "metadata": {},
   "source": [
    "## Data Preprocessing Step:\n",
    "\n",
    "1. **Group Data by Review IDs**: In this step, we group the data by review IDs. This allows us to analyze and process the data at the reviewer level.\n",
    "\n",
    "2. **Split Data into Train and Test Sets**: Once the data is grouped by review IDs, we split the total dataset into train and test subsets. The train set will be used to train our model, while the test set will be used to evaluate its performance.\n",
    "\n",
    "3. **Create Train and Test Sets using surprise's Reader() Class**: To prepare the data for further analysis, we utilize surprise's Reader() class. This class helps parse through the dataframes and create train and test sets that can be used with various collaborative filtering algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c470e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tain_and_test(dataset):\n",
    "    data_to_sample = dataset.copy()\n",
    "    grouped_data = data_to_sample.groupby(by='reviewerID')\n",
    "    train_data = grouped_data.sample(frac=0.8)\n",
    "    test_data = data_to_sample.drop(train_data.index)\n",
    "    reader = Reader(rating_scale=(1,5))\n",
    "    train_data = Dataset.load_from_df(train_data[['reviewerID','asin','overall']], reader = reader)\n",
    "    test_data = Dataset.load_from_df(test_data[['reviewerID','asin','overall']], reader = reader)\n",
    "    trainset = train_data.build_full_trainset()\n",
    "    testset = test_data.build_full_trainset().build_testset()\n",
    "    return trainset, testset, train_data, test_data\n",
    "\n",
    "trainset, testset, train_data, test_data = get_tain_and_test(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23dfad4",
   "metadata": {},
   "source": [
    "## Model training and evaluation:\n",
    "\n",
    "After preparing the datasets for training and testing, I proceeded to evaluate the performance of four recommendation algorithms: SVD, BaselineOnly, NMF, and CoClustering. The implementation of these algorithms was done using the Python package 'Surprise'.\n",
    "\n",
    "The evaluation process involved the following steps:\n",
    "\n",
    "1. **Initialization**: I initialized an instance of each algorithm using the Surprise package. This step sets up the algorithm with default or specified parameters.\n",
    "\n",
    "2. **Training and Testing**: The initialized algorithms were trained using the training set, which contains user-item interactions. The models were then tested using the testing set, which consists of unseen user-item pairs. The algorithms utilized the training data to learn patterns and make predictions for the testing data.\n",
    "\n",
    "3. **Performance Metrics**: To assess the accuracy of the models, I calculated the Root Mean Squared Error (RMSE) and Mean Squared Error (MSE) scores on the predictions obtained from the test set. These metrics provide insights into the quality of the recommendations made by the models.\n",
    "\n",
    "4. **Top 10 Recommendations**: After obtaining the predictions, I selected the top 10 recommendations for each user from each algorithm's output. These recommendations represent the most highly recommended items for each user based on the trained models.\n",
    "\n",
    "5. **Recommendation Evaluation**: Additionally, I evaluated the quality of the recommendation predictions for each algorithm. Specifically, I calculated precision, recall, F-measure, and conversion rate for the top 10 recommendations generated by each algorithm for each item in the test set. The code for precision and recall calculations was adapted from the Surprise package's documentation, which can be found at [this source](https://surprise.readthedocs.io/en/stable/FAQ.html).\n",
    "\n",
    "The models were trained using the Surprise package, which provides a convenient framework for building and evaluating recommendation systems. The use of existing code for precision and recall calculations from the Surprise documentation ensures accurate and standardized evaluation of the recommendation algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edfdc512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(algo_name):\n",
    "    if algo_name == 'Baseline':\n",
    "        return BaselineOnly({'method': 'sgd', 'n_epochs': 10},verbose=True)\n",
    "    elif algo_name == 'SVD':\n",
    "        return SVD()\n",
    "    elif algo_name == 'Co-Clustering':\n",
    "        return CoClustering()\n",
    "    elif algo_name == 'NMF':\n",
    "        return NMF()\n",
    "    else:\n",
    "        print(\"Invalid algorithm!\")\n",
    "\n",
    "def train_model(model_name, train_set, test_set):\n",
    "    print()\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('Current model:', model_name)\n",
    "    model = get_model(model_name)\n",
    "    print('Fitting model...')\n",
    "    model.fit(train_set)\n",
    "    print('Completed model training... ')\n",
    "    print()\n",
    "    print('Obtaining predictions on TEST data... ')\n",
    "    test_predictions = model.test(test_set)\n",
    "    test_rmse = accuracy.rmse(test_predictions, verbose = True)\n",
    "    test_mse = accuracy.mse(test_predictions, verbose = True)\n",
    "    print('test RMSE: ' + str(test_rmse) + ' .... Test MSE: ' + str(test_mse))\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print()\n",
    "    return model, test_predictions\n",
    "\n",
    "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
    "    user_est_true = defaultdict(list)\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold)) for (est, true_r) in user_ratings[:k])\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "    precision = sum(prec for prec in precisions.values()) / len(precisions)\n",
    "    recall = sum(rec for rec in recalls.values()) / len(recalls)\n",
    "    f_measure = (2*recall*precision)/(recall+precision)\n",
    "    c = len([prec for prec in precisions.values() if prec>0])\n",
    "    conversionRate = c/len(precisions)\n",
    "    return precision, recall, f_measure, conversionRate\n",
    "\n",
    "def get_top_k(predictions, k, model_name):\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:k]\n",
    "    File = open('./results/' + str(model_name) + '.txt', \"w\")\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        for i, (iid, est) in enumerate(user_ratings):\n",
    "            File.write(uid + ' ' + iid + \"\\n\")\n",
    "    File.close()\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46edb019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------------------\n",
      "Current model: Baseline\n",
      "Fitting model...\n",
      "Estimating biases using sgd...\n",
      "Completed model training... \n",
      "\n",
      "Obtaining predictions on TEST data... \n",
      "RMSE: 0.9740\n",
      "MSE: 0.9487\n",
      "test RMSE: 0.9739877430272629 .... Test MSE: 0.9486521235673414\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "Precision: 0.9458333333333333\n",
      "Recall: 1.0\n",
      "F-Measure: 0.9721627408993576\n",
      "Conversion Rate: 0.9458333333333333\n"
     ]
    }
   ],
   "source": [
    "baseline_model, baseline_test_predictions = train_model('Baseline', trainset, testset)\n",
    "pickle.dump( baseline_model, open( \"./results/baseline_model.p\", \"wb\" ) )\n",
    "pickle.dump( baseline_test_predictions, open( \"./results/baseline_test_predictions.p\", \"wb\" ) )\n",
    "\n",
    "precision_baseline, recall_baseline, f_measure_baseline, conversionRate_baseline = precision_recall_at_k(baseline_test_predictions, k=10, threshold=3)\n",
    "\n",
    "print('Precision:', precision_baseline)\n",
    "print('Recall:', recall_baseline)\n",
    "print('F-Measure:', f_measure_baseline)\n",
    "print('Conversion Rate:', conversionRate_baseline)\n",
    "\n",
    "baseline_top_recs = get_top_k(baseline_test_predictions, 10, 'baseline_recs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebf9ef97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------------------\n",
      "Current model: SVD\n",
      "Fitting model...\n",
      "Completed model training... \n",
      "\n",
      "Obtaining predictions on TEST data... \n",
      "RMSE: 0.9669\n",
      "MSE: 0.9348\n",
      "test RMSE: 0.9668528793004127 .... Test MSE: 0.9348044902114985\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "Precision: 0.9458333333333333\n",
      "Recall: 1.0\n",
      "F-Measure: 0.9721627408993576\n",
      "Conversion Rate: 0.9458333333333333\n"
     ]
    }
   ],
   "source": [
    "svd_model, svd_test_predictions = train_model('SVD', trainset, testset)\n",
    "pickle.dump( svd_model, open( \"./results/svd_model.p\", \"wb\" ) )\n",
    "pickle.dump( svd_test_predictions, open( \"./results/svd_test_predictions.p\", \"wb\" ) )\n",
    "\n",
    "svd_precision, svd_recall, svd_f_measure, svd_conversionRate = precision_recall_at_k(svd_test_predictions, k=10, threshold=3)\n",
    "\n",
    "print('Precision:', svd_precision)\n",
    "print('Recall:', svd_recall)\n",
    "print('F-Measure:', svd_f_measure)\n",
    "print('Conversion Rate:', svd_conversionRate)\n",
    "\n",
    "svd_top_recs = get_top_k(svd_test_predictions, 10, 'svd_recs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "946f8297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------------------\n",
      "Current model: Co-Clustering\n",
      "Fitting model...\n",
      "Completed model training... \n",
      "\n",
      "Obtaining predictions on TEST data... \n",
      "RMSE: 1.2828\n",
      "MSE: 1.6456\n",
      "test RMSE: 1.2827916545823057 .... Test MSE: 1.6455544290660096\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "Precision: 0.9583333333333334\n",
      "Recall: 0.8958333333333334\n",
      "F-Measure: 0.9260299625468166\n",
      "Conversion Rate: 0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "co_clustering_model, co_clustering_test_predictions = train_model('Co-Clustering', trainset, testset)\n",
    "pickle.dump( co_clustering_model, open( \"./results/co_clustering_model.p\", \"wb\" ) )\n",
    "pickle.dump( co_clustering_model, open( \"./results/co_clustering_test_predictions.p\", \"wb\" ) )\n",
    "\n",
    "co_clustering_precision, co_clustering_recall, co_clustering_f_measure, co_clustering_conversionRate = precision_recall_at_k(co_clustering_test_predictions, k=10, threshold=3)\n",
    "\n",
    "print('Precision:', co_clustering_precision)\n",
    "print('Recall:', co_clustering_recall)\n",
    "print('F-Measure:', co_clustering_f_measure)\n",
    "print('Conversion Rate:', co_clustering_conversionRate)\n",
    "\n",
    "co_clustering_top_recs = get_top_k(co_clustering_test_predictions, 10, 'co_clustering_recs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ffb313f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------------------\n",
      "Current model: NMF\n",
      "Fitting model...\n",
      "Completed model training... \n",
      "\n",
      "Obtaining predictions on TEST data... \n",
      "RMSE: 1.2740\n",
      "MSE: 1.6231\n",
      "test RMSE: 1.2740038591371974 .... Test MSE: 1.6230858330964721\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "Precision: 0.8958333333333334\n",
      "Recall: 0.6916666666666667\n",
      "F-Measure: 0.780621172353456\n",
      "Conversion Rate: 0.8958333333333334\n"
     ]
    }
   ],
   "source": [
    "nmf_model, nmf_test_predictions = train_model('NMF', trainset, testset)\n",
    "pickle.dump( nmf_model, open( \"./results/nmf_model.p\", \"wb\" ) )\n",
    "pickle.dump( nmf_test_predictions, open( \"./results/nmf_test_predictions.p\", \"wb\" ) )\n",
    "\n",
    "nmf_precision, nmf_recall, nmf_f_measure, nmf_conversionRate = precision_recall_at_k(nmf_test_predictions, k=10, threshold=4)\n",
    "\n",
    "print('Precision:', nmf_precision)\n",
    "print('Recall:', nmf_recall)\n",
    "print('F-Measure:', nmf_f_measure)\n",
    "print('Conversion Rate:', nmf_conversionRate)\n",
    "\n",
    "nmf_top_recs = get_top_k(nmf_test_predictions, 10, 'nmf_recs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79d9f04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
